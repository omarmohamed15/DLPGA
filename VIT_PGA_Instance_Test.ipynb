{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs import *\n",
    "from utils import *\n",
    "dat = np.load('Test_PGA_instance.npz')\n",
    "x_test = dat['x_test']\n",
    "y_test = dat['y_test']\n",
    "\n",
    "# Extract Max Value in Each Record.\n",
    "mate = np.max(np.max(np.abs(x_test),axis=1),axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ViT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_classifier(inputs):\n",
    "    \n",
    "    filters = [64, 128, 96, 128, 256] \n",
    "\n",
    "    e = convF1(inputs, filters[1], 3, 0.1)\n",
    "    \n",
    "    e = convF1(e, filters[1], 3, 0.1)\n",
    "\n",
    "    inputreshaped = layers.Reshape((400,1,128))(e)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(inputreshaped)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "    \n",
    "    \n",
    "    #print(patches, encoded_patches)\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        #x3 = convF1(x3, projection_dim,11, 0.1)\n",
    "\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "\n",
    "    return representation\n",
    "\n",
    "inp1 = layers.Input(shape=(400,3),name='input_layer')\n",
    "inp2 = layers.Input(shape=(1,),name='input_layer1')\n",
    "\n",
    "features = create_vit_classifier(inp1)\n",
    "features = concatenate([features,inp2])\n",
    "\n",
    "e = Dense(1)(features)\n",
    "o = Activation('linear', name='output_layer')(e)\n",
    "model = Model(inputs=[inp1,inp2], outputs=o)\n",
    "model.summary()\n",
    "\n",
    "model = load_model('best_model_RR3_3s_1sbefore_Mix100.h5', custom_objects={ \"PatchEncoder\":PatchEncoder,\n",
    "                                                                                   \"Patches\":Patches,\n",
    "                                                                                   \"mlp\":mlp})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Predict Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outte = model.predict([x_test,mate],batch_size=1024,verbose=1)\n",
    "outte = outte[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Prediction Vs Ground-Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig4, ax = plt.subplots()\n",
    "ax.scatter(y_test, outte, alpha = 0.4, facecolors='none', edgecolors='r')\n",
    "ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', alpha=0.4, lw=2)\n",
    "ax.set_xlim([0,340])\n",
    "ax.set_xlabel('PGA (Ground Truth)',fontsize='large', fontweight='bold')\n",
    "ax.set_ylabel('PGA (Predicted)',fontsize='large', fontweight='bold')\n",
    "\n",
    "diff = y_test - outte\n",
    "diff = np.array(diff)\n",
    "(np.round(np.mean(np.abs(diff)), 2)), (np.round(np.std(diff), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Histogrma of the Prediction Error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([diff],100, alpha=0.8,color='lightcoral', edgecolor='black',linewidth=1.2)\n",
    "plt.xlim([-20,20])\n",
    "plt.xlabel('PGA (cm/s/s)',fontsize='large', fontweight='bold')\n",
    "plt.ylabel('Count',fontsize='large', fontweight='bold')\n",
    "plt.text(\n",
    "    0.8,\n",
    "    0.7,\n",
    "    'MAE = ' + str(np.round(np.mean(np.abs(diff)),3)),\n",
    "    horizontalalignment='center',\n",
    "    transform=plt.gca().transAxes,\n",
    "    fontsize=\"medium\",\n",
    "    fontweight=\"bold\")\n",
    "plt.text(\n",
    "    0.8,\n",
    "    0.5,\n",
    "    'STD = ' + str(np.round(np.std((diff)),3)),\n",
    "    horizontalalignment='center',\n",
    "    transform=plt.gca().transAxes,\n",
    "    fontsize=\"medium\",\n",
    "    fontweight=\"bold\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
